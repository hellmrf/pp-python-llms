{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geração Aumentada por Recuperação (RAG)\n",
    "\n",
    "*Retrieval-Augmented Generation* (RAG) é uma técnica que \"enriquece\" o contexto fornecido a um Modelo de Linguagem de Grande Escala (LLM) antes do processamento do prompt. Essa abordagem permite que o modelo se concentre nas informações apresentadas, gerando respostas mais precisas e contextualmente relevantes.\n",
    "\n",
    "Um exemplo prático de RAG é o [NotebookLM](https://notebooklm.google), uma ferramenta do Google projetada para analisar grandes quantidades de texto em arquivos, facilitando a extração de insights ([veja nosso vídeo](https://youtu.be/9Ab5iXh_5c8) sobre isso).\n",
    "\n",
    "## Arquitetura RAG\n",
    "\n",
    "De forma resumida, o processo começa com a extração de uma \"consulta\" (*query*) do prompt inicial. Esta consulta é então utilizada para realizar uma busca em um banco de dados vetorial, que atua como a \"fonte de conhecimento\", de onde são extraídas informações relevantes para enriquecer o contexto.\n",
    "\n",
    "Após a recuperação e integração dessas informações adicionais, o prompt enriquecido é enviado ao LLM, que gera a resposta final. O diagrama abaixo ilustra essa arquitetura de forma visual.\n",
    "\n",
    "![A imagem apresenta um diagrama de fluxo que descreve um processo de interação com um modelo de linguagem de grande escala (LLM).](https://docs.aws.amazon.com/images/sagemaker/latest/dg/images/jumpstart/jumpstart-fm-rag.jpg)\n",
    "\n",
    "[Fonte da imagem](https://aws.amazon.com/pt/what-is/retrieval-augmented-generation/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOLD = \"\\033[1m\"; RESET = \"\\033[0m\"; GREEN = \"\\033[32m\"; BLUE = \"\\033[34m\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Fontes de conhecimento\n",
    "\n",
    "Para cada tipo de aplicação, é desejável utilizar a arquitetura de banco de dados que melhor representa os dados do problema. Neste caso, em que o objetivo é recuperar informações via contexto, o melhor é utilizar um banco de dados vetorial, como o [ChromaDB](https://trychroma.com). Esses bancos de dados armazenam as informações na forma de vetores, de forma que vetores que descrevem temas correlacionados estão próximos (em termos da distância euclidiana) entre si."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mEncontrei 347 arquivos.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "datasource = list(Path(\"./data/aws-case-studies-and-blogs/files\").glob(\"**/*.txt\"))\n",
    "print(f\"Encontrei {len(datasource)} arquivos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# Create a new database\n",
    "client = chromadb.PersistentClient(path=\"chromadb\")\n",
    "coll = client.get_or_create_collection(\"aws-case-studies-and-blogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Li 347 arquivos.\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "import tqdm\n",
    "\n",
    "text_splitter = SemanticChunker(OpenAIEmbeddings())\n",
    "\n",
    "\n",
    "\n",
    "files = []\n",
    "\n",
    "for path in datasource:\n",
    "    content = path.read_text()\n",
    "    files.append(content)\n",
    "\n",
    "print(f\"Li {len(files)} arquivos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvando no Banco de Dados\n",
    "A célula a seguir vai armazenar os documentos carregados no Chroma DB. Na minha máquina, para 347 arquivos, levou 7 min (x documentos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection already contains 0 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding documents: 100%|██████████| 360/360 [05:53<00:00,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 1477 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "count = coll.count()\n",
    "print(f\"A coleção já contém {count} documentos.\")\n",
    "\n",
    "\n",
    "batch_size = 30\n",
    "# Load the documents in batches of batch_size\n",
    "for i in tqdm.tqdm(\n",
    "    range(0, len(files), batch_size), desc=\"Adicionando documentos\", unit_scale=batch_size\n",
    "):\n",
    "\n",
    "    docs = text_splitter.create_documents(files[i : i + batch_size])\n",
    "    ids = [f\"id{i}-{j}\" for j in range(len(docs))]\n",
    "    coll.add(\n",
    "        ids=ids,\n",
    "        documents=[d.page_content for d in docs],\n",
    "    )\n",
    "\n",
    "new_count = coll.count()\n",
    "print(f\"Adicionei {new_count - count} documentos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazendo pesquisas na base de dados\n",
    "\n",
    "Agora que já populamos a base de dados, podemos fazer perguntas especializadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = coll.query(\n",
    "    query_texts=[\"interpretação de argumentos de linha de comando\"],\n",
    "    n_results=10,\n",
    ")\n",
    "\n",
    "search[\"documents\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O resultado com a docs do Python não foi satisfatório."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coraldigital",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
