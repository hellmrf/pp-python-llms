{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baixar o conjunto de dados\n",
    "\n",
    "Ao executar as células a seguir, o conjunto de dados [harshsinghal/nlp-and-llm-related-arxiv-papers](https://www.kaggle.com/datasets/harshsinghal/nlp-and-llm-related-arxiv-papers), que contém artigos do [arXiv](https://arxiv.org/) relacionados a Processamento de Linguagem Natural (NLP) e Modelos de Linguagem de Grande Escala (LLM).\n",
    "\n",
    "O conjunto ~ 970MB em arquivos PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando a API do Kaggle\n",
    "\n",
    "Caso você prefira, pode usar a API do Kaggle. Será necessário seguir a [documentação](https://github.com/Kaggle/kaggle-api/blob/main/docs/README.md#api-credentials): \n",
    "\n",
    "1. baixe o arquivo `kaggle.json` em https://www.kaggle.com/settings/account (na seção API).\n",
    "2. Coloque-o em: \n",
    "   1. Windows: `C:\\Users\\<Windows-username>\\.kaggle\\kaggle.json`\n",
    "   2. Linux: `$XDG_CONFIG_HOME/kaggle/kaggle.json` (em geral será `~/.config/kaggle/kaggle.json`).\n",
    "   3. Outros: `~/.kaggle/kaggle.json`\n",
    "3. Execute as células a seguir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "if not next(Path(\"./data/arxiv/files\").iterdir(), None):\n",
    "    import kaggle\n",
    "\n",
    "    kaggle.api.authenticate()\n",
    "\n",
    "    Path(\"./data/arxiv/files\").mkdir(parents=True, exist_ok=True)\n",
    "    kaggle.api.dataset_download_files('harshsinghal/nlp-and-llm-related-arxiv-papers', path='data/arxiv/files', unzip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sem usar a API do Kaggle\n",
    "\n",
    "Caso você prefira, pode baixar o conjunto de dados [aqui](https://www.kaggle.com/datasets/harshsinghal/nlp-and-llm-related-arxiv-papers). Descompacte-o para `/data/arxiv/files`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langchain_community langchain_chroma pypdf \"unstructured[pdf]\" pytesseract "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 39 0 (offset 0)\n",
      "Ignoring wrong pointing object 61 0 (offset 0)\n",
      "Ignoring wrong pointing object 63 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mLoaded 388 documents.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "# Terminal ANSI colors\n",
    "BOLD = \"\\033[1m\"; RESET = \"\\033[0m\"; GREEN = \"\\033[32m\"; BLUE = \"\\033[34m\"; RED = \"\\033[31m\"\n",
    "\n",
    "loader = PyPDFDirectoryLoader(\"./data/arxiv/files/\")\n",
    "print(f\"Loading files...\")\n",
    "docs = loader.load()\n",
    "print(f\"{GREEN}Loaded {len(docs)} documents.{RESET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain_experimental langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mSeparei 388 documentos em 1691 pedaços.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "text_splitter = SemanticChunker(OpenAIEmbeddings())\n",
    "original_docs_len = len(docs)\n",
    "docs = text_splitter.transform_documents(docs)\n",
    "print(f\"{GREEN}Separei {original_docs_len} documentos em {len(docs)} pedaços.{RESET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A coleção está vazia.\n",
      "Começando a adicionar 1691 documentos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adicionando documentos:   0%|          | 0/1710 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adicionando documentos: 100%|██████████| 1710/1710 [02:57<00:00,  9.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adicionei 1691 documentos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import tqdm\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"chromadb\")\n",
    "coll = client.get_or_create_collection(\"arxiv\")\n",
    "\n",
    "count = coll.count()\n",
    "print(f\"A coleção já contém {count} documentos.\" if count else \"A coleção está vazia.\")\n",
    "\n",
    "ids = [f\"id{i}\" for i in range(len(docs))]\n",
    "\n",
    "print(f\"Começando a adicionar {len(docs)} documentos...\")\n",
    "\n",
    "batch_size = 30\n",
    "# Load the documents in batches of batch_size\n",
    "for i in tqdm.tqdm(\n",
    "    range(0, len(docs), batch_size), desc=\"Adicionando documentos\", unit_scale=batch_size\n",
    "):\n",
    "\n",
    "    coll.add(\n",
    "        ids=ids[i : i+batch_size],\n",
    "        documents=[d.page_content for d in docs[i : i + batch_size]],\n",
    "    )\n",
    "\n",
    "new_count = coll.count()\n",
    "print(f\"{GREEN}Adicionei {new_count - count} documentos.{RESET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A pesquisa retornou 7702 caracteres.\n"
     ]
    }
   ],
   "source": [
    "search = coll.query(\n",
    "    query_texts=[\"qual a importância dos llms?\"],\n",
    "    n_results=5,\n",
    ")\n",
    "\n",
    "chars = sum(len(d) for d in search[\"documents\"][0])\n",
    "print(f\"A pesquisa retornou {chars} caracteres.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending 7767 characters to the model...\n",
      "\u001b[1m[User ] >>\u001b[0m qual a importância dos llms?\n",
      "\n",
      "\u001b[1m[Model] >>\u001b[0m O texto que você forneceu destaca a importância dos LLMs (Large Language Models) em diversos aspectos. Vamos resumir os pontos chave:\n",
      "\n",
      "**1. Capacidade e Impacto:**\n",
      "\n",
      "* **Compreensão e Geração de Linguagem:** LLMs demonstram uma capacidade impressionante de entender a linguagem natural e gerar textos coerentes e contextualmente relevantes.\n",
      "* **Resolução de Tarefas Complexas:** Através da geração de texto, os LLMs podem ser utilizados para solucionar uma variedade de tarefas, desde tradução e resumo até programação e escrita criativa.\n",
      "* **Avanços Significativos em IA:** O desenvolvimento de LLMs representa um avanço notável no campo da inteligência artificial, abrindo caminho para novas aplicações e pesquisas.\n",
      "\n",
      "**2. Aplicações Práticas:**\n",
      "\n",
      "* **Pesquisa Científica:** LLMs podem auxiliar em diversas etapas da pesquisa, como revisão de literatura, geração de hipóteses, análise de dados e escrita de artigos.\n",
      "* **Escrita e Comunicação:** LLMs podem ser ferramentas valiosas para escritores, ajudando na geração de ideias, organização de conteúdo, aprimoramento da escrita e até mesmo na criação de diferentes estilos textuais.\n",
      "* **Outras Áreas:** O texto menciona ainda o potencial dos LLMs em áreas como psicologia, demonstrando sua versatilidade e aplicabilidade em múltiplos domínios.\n",
      "\n",
      "**3. Desafios e Oportunidades:**\n",
      "\n",
      "* **Compreensão do Funcionamento:** Apesar dos avanços, ainda há muito a ser compreendido sobre como os LLMs funcionam, especialmente em relação à sua capacidade de aprendizado e resolução de problemas.\n",
      "* **Alinhamento com Valores Humanos:** É crucial garantir que os LLMs sejam desenvolvidos e utilizados de forma ética e responsável, evitando a geração de conteúdo enviesado, prejudicial ou inadequado.\n",
      "* **Democratização do Acesso:**  A pesquisa e o desenvolvimento de LLMs exigem recursos computacionais e financeiros significativos, o que pode gerar disparidades no acesso a essa tecnologia. É importante buscar formas de democratizar o acesso aos LLMs e seus benefícios.\n",
      "\n",
      "Em resumo, os LLMs representam uma poderosa ferramenta com potencial para revolucionar a maneira como interagimos com a linguagem e a informação. As pesquisas nesse campo são extremamente promissoras, mas é fundamental abordar os desafios éticos e práticos para garantir que os LLMs sejam utilizados de forma benéfica para a sociedade. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "context = \"\\n\".join(search[\"documents\"][0])\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-pro\")\n",
    "\n",
    "query = \"qual a importância dos llms?\"\n",
    "prompt = f\"Contexto: {context}\\n\\nPergunta: {query}\\n\\nResposta:\"\n",
    "print(f\"Sending {len(prompt)} characters to the model...\")\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(f\"{BOLD}[User ] >>{RESET} {query}\\n\\n{BOLD}[Model] >>{RESET} {response.content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coraldigital",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
