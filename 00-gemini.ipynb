{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizando Modelos de Linguagem de Grande Escala (LLM) em Python\n",
    "\n",
    "‚úèÔ∏è Por [Heliton Martins](https://hellmrf.dev.br) | 01 de agosto de 2024 | [üå± Programa√ß√£o Popular](https://youtube.com/@programacaopopular).\n",
    "\n",
    "## Google Gemini 1.5\n",
    "\n",
    "Usaremos neste notebook os seguintes modelos (LLMs):\n",
    "- **Google Gemini** (gratuito com limita√ß√µes)\n",
    "    - üóùÔ∏è [Google AI Studio](https://aistudio.google.com/app/prompts/new_chat) (pegue sua chave aqui)\n",
    "    - üìÑ [Documenta√ß√£o](https://ai.google.dev/gemini-api/docs)\n",
    "\n",
    "Diferente do GPT (acess√≠vel pela API da OpenAI), o Gemini √© gratuito para testes, sendo apenas necess√°rio gerar uma chave de API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depend√™ncias\n",
    "\n",
    "Execute a c√©lula a seguir para instalar as depend√™ncias no Google Colab. Caso voc√™ esteja em um ambiente local, pode usar o comando `pip install <pacote>` pelo terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU google-generativeai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credenciais\n",
    "\n",
    "Para configurar suas chaves de API\n",
    "1. Copie o arquivo `.env.sample` para `.env`, e inclua nele suas chaves de API. Esse arquivo **n√£o deve ser commitado**, j√° que cont√©m informa√ß√µes sens√≠veis que permitem a qualquer um usar suas cotas nas APIs.\n",
    "2. Execute a c√©lula a seguir e continue. Caso a chave de API n√£o seja encontrada no arquivo `.env`, elas ser√£o solicitadas.\n",
    "\n",
    "Caso esteja em ambiente local e n√£o queira usar o arquivo `.env` ou o mecanismo de oculta√ß√£o das chaves, basta defin√≠-las explicitamente:\n",
    "\n",
    "```python\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"YOUR-API-KEY\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Gemini API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini: Checando modelos dispon√≠veis\n",
    "\n",
    "O c√≥digo a seguir configura o Gemini usando o [binding oficial para Python](https://pypi.org/project/google-generativeai/), para que ent√£o possamos ver os modelos dispon√≠veis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai # este √© o pacote oficial da Google.\n",
    "import os\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A c√©lula a seguir vai listar todos os modelos dispon√≠veis para a sua chave de API do Gemini. Os mais poderosos s√£o `models/gemini-1.5-pro` (n√£o dispon√≠vel para todos) e `models/gemini-1.5-flash` (publicamente dispon√≠vel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mmodels/chat-bison-001\u001b[0m (PaLM 2 Chat (Legacy))\n",
      "    \u001b[32mA legacy text-only model optimized for chat conversations\u001b[0m\n",
      "    \u001b[1mMax input tokens:\u001b[0m 4096\n",
      "    \u001b[1mMax output tokens:\u001b[0m 1024\n",
      "\u001b[1m\u001b[34mmodels/text-bison-001\u001b[0m (PaLM 2 (Legacy))\n",
      "    \u001b[32mA legacy model that understands text and generates text as an output\u001b[0m\n",
      "    \u001b[1mMax input tokens:\u001b[0m 8196\n",
      "    \u001b[1mMax output tokens:\u001b[0m 1024\n",
      "\u001b[1m\u001b[34mmodels/embedding-gecko-001\u001b[0m (Embedding Gecko)\n",
      "    \u001b[32mObtain a distributed representation of a text.\u001b[0m\n",
      "    \u001b[1mMax input tokens:\u001b[0m 1024\n",
      "    \u001b[1mMax output tokens:\u001b[0m 1\n",
      "\u001b[1m\u001b[34mmodels/gemini-1.0-pro-latest\u001b[0m (Gemini 1.0 Pro Latest)\n",
      "    \u001b[32mThe best model for scaling across a wide range of tasks. This is the latest model.\u001b[0m\n",
      "    \u001b[1mMax input tokens:\u001b[0m 30720\n",
      "    \u001b[1mMax output tokens:\u001b[0m 2048\n",
      "\u001b[1m\u001b[34mmodels/gemini-1.0-pro\u001b[0m (Gemini 1.0 Pro)\n",
      "    \u001b[32mThe best model for scaling across a wide range of tasks\u001b[0m\n",
      "    \u001b[1mMax input tokens:\u001b[0m 30720\n",
      "    \u001b[1mMax output tokens:\u001b[0m 2048\n",
      "\u001b[1m\u001b[34mmodels/gemini-pro\u001b[0m (Gemini 1.0 Pro)\n",
      "    \u001b[32mThe best model for scaling across a wide range of tasks\u001b[0m\n",
      "    \u001b[1mMax input tokens:\u001b[0m 30720\n",
      "    \u001b[1mMax output tokens:\u001b[0m 2048\n",
      "\u001b[1m\u001b[34mmodels/gemini-1.0-pro-001\u001b[0m (Gemini 1.0 Pro 001 (Tuning))\n",
      "    \u001b[32mThe best model for scaling across a wide range of tasks. This is a stable model that supports tuning.\u001b[0m\n",
      "    \u001b[1mMax input tokens:\u001b[0m 30720\n",
      "    \u001b[1mMax output tokens:\u001b[0m 2048\n",
      "\u001b[1m\u001b[34mmodels/gemini-1.0-pro-vision-latest\u001b[0m (Gemini 1.0 Pro Vision)\n",
      "    \u001b[32mThe best image understanding model to handle a broad range of applications\u001b[0m\n",
      "    \u001b[1mMax input tokens:\u001b[0m 12288\n",
      "    \u001b[1mMax output tokens:\u001b[0m 4096\n",
      "\u001b[1m\u001b[34mmodels/gemini-pro-vision\u001b[0m (Gemini 1.0 Pro Vision)\n",
      "    \u001b[32mThe best image understanding model to handle a broad range of applications\u001b[0m\n",
      "    \u001b[1mMax input tokens:\u001b[0m 12288\n",
      "    \u001b[1mMax output tokens:\u001b[0m 4096\n",
      "\u001b[1m\u001b[34mmodels/gemini-1.5-pro-latest\u001b[0m (Gemini 1.5 Pro Latest)\n",
      "    \u001b[32mMid-size multimodal model that supports up to 2 million tokens\u001b[0m\n",
      "    \u001b[1mMax input tokens:\u001b[0m 2097152\n",
      "    \u001b[1mMax output tokens:\u001b[0m 8192\n",
      "\u001b[1m\u001b[34mmodels/gemini-1.5-pro-001\u001b[0m (Gemini 1.5 Pro 001)\n",
      "    \u001b[32mMid-size multimodal model that supports up to 2 million tokens\u001b[0m\n",
      "    \u001b[1mMax input tokens:\u001b[0m 2097152\n",
      "    \u001b[1mMax output tokens:\u001b[0m 8192\n",
      "\u001b[1m\u001b[34mmodels/gemini-1.5-pro\u001b[0m (Gemini 1.5 Pro)\n",
      "    \u001b[32mMid-size multimodal model that supports up to 2 million tokens\u001b[0m\n",
      "    \u001b[1mMax input tokens:\u001b[0m 2097152\n",
      "    \u001b[1mMax output tokens:\u001b[0m 8192\n",
      "\u001b[1m\u001b[34mmodels/gemini-1.5-pro-exp-0801\u001b[0m (Gemini 1.5 Pro Experimental 0801)\n",
      "    \u001b[32mMid-size multimodal model that supports up to 2 million tokens\u001b[0m\n",
      "    \u001b[1mMax input tokens:\u001b[0m 2097152\n",
      "    \u001b[1mMax output tokens:\u001b[0m 8192\n",
      "\u001b[1m\u001b[34mmodels/gemini-1.5-flash-latest\u001b[0m (Gemini 1.5 Flash Latest)\n",
      "    \u001b[32mFast and versatile multimodal model for scaling across diverse tasks\u001b[0m\n",
      "    \u001b[1mMax input tokens:\u001b[0m 1048576\n",
      "    \u001b[1mMax output tokens:\u001b[0m 8192\n",
      "\u001b[1m\u001b[34mmodels/gemini-1.5-flash-001\u001b[0m (Gemini 1.5 Flash 001)\n",
      "    \u001b[32mFast and versatile multimodal model for scaling across diverse tasks\u001b[0m\n",
      "    \u001b[1mMax input tokens:\u001b[0m 1048576\n",
      "    \u001b[1mMax output tokens:\u001b[0m 8192\n",
      "\u001b[1m\u001b[34mmodels/gemini-1.5-flash\u001b[0m (Gemini 1.5 Flash)\n",
      "    \u001b[32mFast and versatile multimodal model for scaling across diverse tasks\u001b[0m\n",
      "    \u001b[1mMax input tokens:\u001b[0m 1048576\n",
      "    \u001b[1mMax output tokens:\u001b[0m 8192\n",
      "\u001b[1m\u001b[34mmodels/embedding-001\u001b[0m (Embedding 001)\n",
      "    \u001b[32mObtain a distributed representation of a text.\u001b[0m\n",
      "    \u001b[1mMax input tokens:\u001b[0m 2048\n",
      "    \u001b[1mMax output tokens:\u001b[0m 1\n",
      "\u001b[1m\u001b[34mmodels/text-embedding-004\u001b[0m (Text Embedding 004)\n",
      "    \u001b[32mObtain a distributed representation of a text.\u001b[0m\n",
      "    \u001b[1mMax input tokens:\u001b[0m 2048\n",
      "    \u001b[1mMax output tokens:\u001b[0m 1\n",
      "\u001b[1m\u001b[34mmodels/aqa\u001b[0m (Model that performs Attributed Question Answering.)\n",
      "    \u001b[32mModel trained to return answers to questions that are grounded in provided sources, along with estimating answerable probability.\u001b[0m\n",
      "    \u001b[1mMax input tokens:\u001b[0m 7168\n",
      "    \u001b[1mMax output tokens:\u001b[0m 1024\n"
     ]
    }
   ],
   "source": [
    "from utils import print_gemini_model\n",
    "\n",
    "for model in genai.list_models():\n",
    "    print_gemini_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini 1.5 Flash em a√ß√£o!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√£o, os c√£es n√£o voam. Eles s√£o mam√≠feros terrestres, o que significa que eles vivem em terra e n√£o t√™m asas para voar. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "response = model.generate_content(\"Os c√£es voam?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com isso, conectamos ao Gemini 1.5 Flash e geramos uma resposta utilizando Python."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coraldigital",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
