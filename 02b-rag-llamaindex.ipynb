{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geração Aumentada por Recuperação (RAG)\n",
    "\n",
    "*Retrieval-Augmented Generation* (RAG) é uma técnica que \"enriquece\" o contexto fornecido a um Modelo de Linguagem de Grande Escala (LLM) antes do processamento do prompt. Essa abordagem permite que o modelo se concentre nas informações apresentadas, gerando respostas mais precisas e contextualmente relevantes.\n",
    "\n",
    "Um exemplo prático de RAG é o [NotebookLM](https://notebooklm.google), uma ferramenta do Google projetada para analisar grandes quantidades de texto em arquivos, facilitando a extração de insights ([veja nosso vídeo](https://youtu.be/9Ab5iXh_5c8) sobre isso).\n",
    "\n",
    "## Arquitetura RAG\n",
    "\n",
    "De forma resumida, o processo começa com a extração de uma \"consulta\" (*query*) do prompt inicial. Esta consulta é então utilizada para realizar uma busca em um banco de dados vetorial, que atua como a \"fonte de conhecimento\", de onde são extraídas informações relevantes para enriquecer o contexto.\n",
    "\n",
    "Após a recuperação e integração dessas informações adicionais, o prompt enriquecido é enviado ao LLM, que gera a resposta final. O diagrama abaixo ilustra essa arquitetura de forma visual.\n",
    "\n",
    "![A imagem apresenta um diagrama de fluxo que descreve um processo de interação com um modelo de linguagem de grande escala (LLM).](https://docs.aws.amazon.com/images/sagemaker/latest/dg/images/jumpstart/jumpstart-fm-rag.jpg)\n",
    "\n",
    "[Fonte da imagem](https://aws.amazon.com/pt/what-is/retrieval-augmented-generation/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala o llama_index, que é um middleware popular usado em muitas aplicações de GenAI\n",
    "%pip install llama_index langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mDescompactando arquivos...\u001b[0m\n",
      "\u001b[32mDescompactação concluída.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Terminal ANSI colors\n",
    "BOLD = \"\\033[1m\"; RESET = \"\\033[0m\"; GREEN = \"\\033[32m\"; BLUE = \"\\033[34m\"; RED = \"\\033[31m\"\n",
    "\n",
    "# Verificar se dados existem\n",
    "data_zip = Path(\"./data/aws-case-studies-and-blogs/files.zip\")\n",
    "source_dir = Path(\"./data/aws-case-studies-and-blogs/files\")\n",
    "if not source_dir.exists():\n",
    "    source_dir.mkdir(parents=True)\n",
    "\n",
    "if not next(source_dir.iterdir(), None):\n",
    "    if data_zip.exists():\n",
    "        print(f\"{GREEN}Descompactando dataset...{RESET}\")\n",
    "        import zipfile\n",
    "        with zipfile.ZipFile(data_zip, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(source_dir)\n",
    "        print(f\"{GREEN}Descompactação concluída.{RESET}\")\n",
    "    else:\n",
    "        print(f\"{RED}Não encontrei os arquivos do dataset.{RESET}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mCarreguei 347 documentos.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data/aws-case-studies-and-blogs/files\").load_data()\n",
    "print(f\"{GREEN}{BOLD}Carreguei {len(documents)} documentos.{RESET}\")\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By utilizing Amazon SageMaker Real-Time Inference functionality, you can generate a large number of images quickly and efficiently. This service allows for the creation of images based on text prompts in a matter of seconds, enabling the generation of a high volume of unique and high-quality images for users.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"Como eu poderia gerar muitas imagens?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coraldigital",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
